from common_styles import apply_common_styles, set_page_config
import auth

import os
import tempfile
import shutil
import re
import streamlit as st
from dotenv import load_dotenv
from datetime import datetime, timedelta
from zoneinfo import ZoneInfo
import math
from typing import List, Tuple


SEOUL_TZ = ZoneInfo("Asia/Seoul")


# === [ADD] ì—¬ëŸ¬ í˜•íƒœì˜ ì„¹ì…˜ í—¤ë” ì¤„ë°”ê¿ˆ ë³´ì • ===
HEADER_MARKERS = ("â€»", "â– ", "â—†", "â—", "â–¶", "â–·", "â–²", "â–¸", "â€¢")
BULLET_MARKERS = ("- ", "â€¢ ", "ã†", "Â·", "* ", "â€” ", "â€“ ")

HEADER_KEYWORDS = (
    "ê°œìš”",
    "ê³µì‚¬ ë²”ìœ„",
    "ê³µì‚¬ë²”ìœ„",
    "ê²¬ì ì¡°ê±´",
    "ì ìš©ë²”ìœ„",
    "UBR ê³µì‚¬ë¶„",
    "ì¬ë£Œ",
    "ìì¬",
    "ì¹˜ìˆ˜",
    "ê·œê²©",
    "ì‹œê³µ ì ˆì°¨",
    "ì‹œê³µì ˆì°¨",
    "í’ˆì§ˆ",
    "ê²€ìˆ˜",
    "ìœ ì˜",
    "ìœ ì˜ì‚¬í•­",
    "ì•ˆì „",
    "ê¸°íƒ€",
    "ì°¸ê³ ",
    "ê·¼ê±°",
)

_hdr_colon_re = re.compile(r"^\s*[^:ï¼š]{1,80}\s*[:ï¼š]\s*$")
_hdr_number_re = re.compile(r"^\s*(ì œ?\d+(?:\.\d+)*[)\.]?)\s+[^\s].{0,60}$")
_hdr_keyword_re = re.compile(
    r"^\s*(" + "|".join(map(re.escape, HEADER_KEYWORDS)) + r")\s*$"
)


def _is_header_line(s: str) -> bool:
    s = s.strip()
    if not s:
        return False
    if s.startswith(HEADER_MARKERS):  # ê¸°í˜¸í˜• í—¤ë”
        return True
    if _hdr_colon_re.match(s):  # ì½œë¡ í˜• í—¤ë” (ì˜ˆ: "ì¬ë£Œ:")
        return True
    if _hdr_number_re.match(s):  # ë²ˆí˜¸í˜• í—¤ë” (ì˜ˆ: "1. ê°œìš”", "ì œ2. í’ˆì§ˆ")
        return True
    if _hdr_keyword_re.match(s):  # í‚¤ì›Œë“œ ë‹¨ë… í—¤ë” (ì˜ˆ: "ê³µì‚¬ ë²”ìœ„")
        return True
    return False


def _is_bullet_line(s: str) -> bool:
    ss = s.lstrip()
    return any(ss.startswith(m) for m in BULLET_MARKERS)


def _normalize_multiline_sections_enhanced(text: str) -> str:
    """
    PDF ì¶”ì¶œ ê³¼ì •ì—ì„œ í—¤ë”ê°€ ì¤„ë°”ê¿ˆìœ¼ë¡œ ëŠê¸´ ê²ƒì„ ë³´ì •.
    - í—¤ë” ë¼ì¸ ì¸ì‹(ê¸°í˜¸/ì½œë¡ /ë²ˆí˜¸/í‚¤ì›Œë“œ)
    - ë°”ë¡œ ë‹¤ìŒ ë¼ì¸ì´ ë¶ˆë¦¿/í—¤ë”/ë¹ˆì¤„ì´ ì•„ë‹ˆê³  ë„ˆë¬´ ê¸¸ì§€ ì•Šìœ¼ë©´(<=120ì) ìµœëŒ€ 3ì¤„ê¹Œì§€ ì´ì–´ë¶™ì„.
    """
    lines = text.splitlines()
    out = []
    buf = None
    tail_joined = 0

    for raw in lines:
        s = raw.strip()

        if _is_header_line(s):
            if buf is not None:
                out.append(buf)
            buf = s
            tail_joined = 0
            continue

        if buf is not None:
            if (
                s
                and not _is_header_line(s)
                and not _is_bullet_line(s)
                and len(s) <= 120
                and tail_joined < 3
            ):
                buf += " " + s
                tail_joined += 1
                continue
            else:
                out.append(buf)
                buf = None
                tail_joined = 0

        out.append(raw)

    if buf is not None:
        out.append(buf)

    return "\n".join(out)


# === [/ADD] ===


# LangChain (ìµœì‹  êµ¬ì¡°)
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import PyPDFLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.documents import Document

# ---------------------------------------
# í™˜ê²½ì„¤ì •
# ---------------------------------------
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
assert OPENAI_API_KEY, "OPENAI_API_KEYê°€ .envì— ì—†ìŠµë‹ˆë‹¤."

set_page_config(page_title="ì‹œë°©ì„œ Q&A ì±—ë´‡", page_icon="ğŸ›", layout="wide")
apply_common_styles()

auth.require_auth()

st.title("ğŸ› ì‹œë°©ì„œ Q&A ì±—ë´‡")

# ---------------------------------------
# âœ… ìƒíƒœ ì´ˆê¸°í™”
# ---------------------------------------
if "vectorstore" not in st.session_state:
    st.session_state["vectorstore"] = None
if "chat_history" not in st.session_state:
    st.session_state["chat_history"] = []
# ìƒˆë¡œ ì¶”ê°€: ë§ˆì§€ë§‰ ì¸ë±ìŠ¤ ë°°ì¹˜ì™€ ê·¸ ìš”ì•½
if "last_index_batch_docs" not in st.session_state:
    st.session_state["last_index_batch_docs"] = []
if "last_index_summary" not in st.session_state:
    st.session_state["last_index_summary"] = None

# ---------------------------------------
# ì‚¬ì´ë“œë°”: ëª¨ë¸/ì˜µì…˜
# ---------------------------------------
with st.sidebar:
    st.markdown("### âš™ï¸ ì˜µì…˜")
    model_name = "gpt-5-mini"
    st.markdown("âš™ï¸ LLM ëª¨ë¸: gpt-5")
    k_ctx = st.slider("ê²€ìƒ‰ ë¬¸ì„œ ìˆ˜(k)", 2, 8, 4, 1)
    chunk_size = st.slider("ì²­í¬ í¬ê¸°", 500, 2000, 1000, 100)
    chunk_overlap = st.slider("ì˜¤ë²„ë©", 50, 400, 150, 25)
    st.markdown("---")
    st.markdown("**íŒŒì¼ ì—…ë¡œë“œ í›„, [ì¸ë±ìŠ¤ ìƒì„±]ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.**")


# ---------------------------------------
# ê³µìš©: ì—…ë¡œë“œ íŒŒì¼ì„ ì„ì‹œê²½ë¡œë¡œ ì €ì¥
# ---------------------------------------
def _save_uploaded_to_temp(uploaded_file, suffix):
    """Streamlit UploadedFile -> temp file path"""
    tmp = tempfile.NamedTemporaryFile(delete=False, suffix=suffix)
    try:
        shutil.copyfileobj(uploaded_file, tmp)
        tmp.flush()
        return tmp.name
    finally:
        tmp.close()


# ---------------------------------------
# í•¨ìˆ˜: ìµœì‹ ìš°ì„  ê°€ì¤‘ì¹˜(ì‹ ì„ ë„) + ìœ ì‚¬ë„ ì¬ë­í‚¹ ê²€ìƒ‰
# ---------------------------------------


def _parse_ts(ts: str) -> float:
    # ISO8601 â†’ epoch seconds
    try:
        return datetime.fromisoformat(ts).timestamp()
    except Exception:
        return 0.0


def search_with_recency_rerank(
    vs,
    query: str,
    k: int = 4,
    fetch_k: int = 32,
    w_recency: float = 0.35,
    half_life_days: float = 14.0,
) -> List[Document]:
    """
    ë²¡í„° ìœ ì‚¬ë„ + ì‹ ì„ ë„(ì§€ìˆ˜ê°ì‡ ) ê²°í•© ì ìˆ˜ë¡œ ì¬ë­í¬.
    FAISS.similarity_search_with_score ë¥¼ ì‚¬ìš©í•˜ê³ , ì ìˆ˜ì •ê·œí™” í›„ ê²°í•©.
    """
    # 1) ì¶©ë¶„íˆ ë„“ê²Œ í›„ë³´ ìˆ˜ì§‘
    try:
        pairs: List[Tuple[Document, float]] = vs.similarity_search_with_score(
            query, k=fetch_k
        )
        # ì¼ë¶€ êµ¬í˜„ì€ scoreê°€ "ì‘ì„ìˆ˜ë¡ ìœ ì‚¬"(ê±°ë¦¬)ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë’¤ì—ì„œ ì •ê·œí™”ë¡œ ë³´ì •
    except Exception:
        # fallback
        docs = vs.similarity_search(query, k=fetch_k)
        pairs = [(d, 0.0) for d in docs]

    now = datetime.now(tz=SEOUL_TZ).timestamp()
    # 2) score ì •ê·œí™” (min-max â†’ ìœ ì‚¬ë„ ë°©í–¥ìœ¼ë¡œ ë’¤ì§‘ê¸°)
    scores = [s for _, s in pairs]
    if scores:
        s_min, s_max = min(scores), max(scores)
        # ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜: ì‘ì€ê²Œ ë” ìœ ì‚¬ â†’ inv_norm
        sim_norm = []
        for doc, s in pairs:
            if s_max == s_min:
                inv = 1.0
            else:
                # 0~1ë¡œ ì •ê·œí™” í›„ ë’¤ì§‘ê¸°
                inv = 1.0 - ((s - s_min) / (s_max - s_min))
            sim_norm.append((doc, inv))
    else:
        sim_norm = [(doc, 1.0) for doc, _ in pairs]

    # 3) recency ì ìˆ˜: half-life ê¸°ë°˜ ì§€ìˆ˜ ê°ì‡ 
    hl_secs = half_life_days * 86400.0
    ranked = []
    for doc, sim in sim_norm:
        ts = _parse_ts(doc.metadata.get("timestamp", ""))  # epoch
        # ì‹œê°„ì´ ì—†ìœ¼ë©´ 0ì 
        if ts <= 0:
            rec = 0.0
        else:
            age = max(0.0, now - ts)
            rec = math.exp(-age / hl_secs)  # ìµœê·¼ì¼ìˆ˜ë¡ 1ì— ê°€ê¹Œì›€

        combined = (1.0 - w_recency) * sim + (w_recency) * rec
        ranked.append((combined, doc, sim, rec))

    ranked.sort(key=lambda x: x[0], reverse=True)
    return [d for _, d, _, _ in ranked[:k]]


# ---------------------------------------
# í•¨ìˆ˜: ë¬¸ì„œ ë¡œë”© (PDF/Text)
# ---------------------------------------
def load_docs(uploaded_files):
    docs = []
    batch_id = datetime.now(tz=SEOUL_TZ).strftime("%Y%m%d-%H%M%S")
    base_ts = datetime.now(tz=SEOUL_TZ)
    step = 1  # íŒŒì¼ ê°„ 1ì´ˆ ê°„ê²©

    for idx, f in enumerate(uploaded_files):
        suffix = os.path.splitext(f.name)[1].lower()
        file_ts = (base_ts - timedelta(seconds=step * idx)).isoformat()

        if suffix == ".pdf":
            tmp_path = _save_uploaded_to_temp(f, ".pdf")
            try:
                loader = PyPDFLoader(tmp_path)
                loaded = loader.load()
                for d in loaded:
                    d.metadata["display_name"] = f.name
                    d.metadata["batch_id"] = batch_id
                    d.metadata["timestamp"] = file_ts
                    d.page_content = _normalize_multiline_sections_enhanced(
                        d.page_content
                    )
                docs.extend(loaded)
            finally:
                os.unlink(tmp_path)

        elif suffix in [".txt", ".md"]:
            tmp_path = _save_uploaded_to_temp(f, suffix)
            try:
                loader = TextLoader(tmp_path, encoding="utf-8")
                loaded = loader.load()
                for d in loaded:
                    d.metadata["display_name"] = f.name
                    d.metadata["batch_id"] = batch_id
                    d.metadata["timestamp"] = file_ts
                    d.page_content = _normalize_multiline_sections_enhanced(
                        d.page_content
                    )
                docs.extend(loaded)
            finally:
                os.unlink(tmp_path)
        else:
            st.warning(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” í˜•ì‹: {f.name}")

    return docs


# ---------------------------------------
# í•¨ìˆ˜: ì²­í¬ ë¶„í• 
# ---------------------------------------
def split_docs(docs, chunk_size=1000, chunk_overlap=150):
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        separators=["\n\n", "\n", " ", ""],
    )
    return splitter.split_documents(docs)


# ---------------------------------------
# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ìš•ì‹¤ ê³µì‚¬ ì‹œë°©ì„œ ì „ìš©)
# ---------------------------------------
SYSTEM_INSTRUCTIONS = """\
ë„ˆëŠ” ìš•ì‹¤(UBR) ê³µì‚¬ ì‹œë°©ì„œ ì „ìš© ì „ë¬¸ê°€ ì–´ì‹œìŠ¤í„´íŠ¸ë‹¤.
- ë°˜ë“œì‹œ ì—…ë¡œë“œëœ ì‹œë°©ì„œ/ë„ë©´(ì»¨í…ìŠ¤íŠ¸)ì— ê·¼ê±°í•´ ëŒ€ë‹µí•˜ë¼.
- ê·¼ê±°ê°€ ë¶ˆì¶©ë¶„í•˜ë©´ 'í•´ë‹¹ì‚¬í•­ ì—†ìŒ' ë˜ëŠ” 'ì‹œë°©ì„œì— ëª…ì‹œ ì—†ìŒ'ì´ë¼ê³  ë‹µí•˜ê³  ì¶”ì¸¡í•˜ì§€ ë§ˆë¼.
- ì§ˆë¬¸ì´ ì‹œë°©ì„œ ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ë©´ 'ë³¸ ì±—ë´‡ì€ ì‹œë°©ì„œ ê¸°ë°˜ ì§ˆì˜ë§Œ ë‹µë³€í•©ë‹ˆë‹¤'ë¼ê³  ì•ˆë‚´í•˜ë¼.
- ìˆ˜ëŸ‰ì´ë‚˜ ì¹˜ìˆ˜ ê³„ì‚°ì´ í•„ìš”í•œ ê²½ìš°, ë¬¸ì„œ ê·¼ê±°(í˜ì´ì§€/ë¬¸êµ¬)ë¥¼ ìš”ì•½í•´ì„œ í•¨ê»˜ ì œì‹œí•˜ë¼.
- ë‹µë³€ì€ í•œêµ­ì–´ë¡œ, í•­ëª©í˜•/í‘œí˜• ì •ë¦¬ ì„ í˜¸.
"""

USER_PROMPT = ChatPromptTemplate.from_messages(
    [
        ("system", SYSTEM_INSTRUCTIONS),
        (
            "human",
            """\
ë‹¤ìŒì€ ê²€ìƒ‰ëœ ì‹œë°©ì„œ ì»¨í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤. ì´ë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µí•˜ë¼.

[ì»¨í…ìŠ¤íŠ¸]
{context}

[ëŒ€í™” íˆìŠ¤í† ë¦¬ ìš”ì•½]
{chat_history}

[ì§ˆë¬¸]
{question}

ìš”êµ¬ì‚¬í•­:
- ë¬¸ì„œ ê·¼ê±°ì˜ í•µì‹¬ ë¬¸êµ¬ë¥¼ ì¸ìš©(ìš”ì•½)í•˜ê³ , ê°€ëŠ¥í•œ ê²½ìš° í˜ì´ì§€/ì„¹ì…˜ì„ í•¨ê»˜ ì œì‹œ.
- ëª¨í˜¸í•˜ë©´ ëª…ì‹œì ìœ¼ë¡œ 'í•´ë‹¹ì‚¬í•­ ì—†ìŒ' ê¸°ì¬.
- ìµœì¢…ì— 'ìš”ì•½' ì„¹ì…˜ìœ¼ë¡œ 3ì¤„ ì´ë‚´ í•µì‹¬ë§Œ ì¬ì •ë¦¬.
""",
        ),
    ]
)

# -------------------------------
# ğŸ”´ ìš”ì (ë³¼ë“œ/ê²½ê³ ) ì¶”ì¶œ ìœ í‹¸
# -------------------------------
HIGHLIGHT_PATTERNS = [
    r"\*\*(.+?)\*\*",  # **bold**
    r"(?:\(|\[|ã€)?\s*ì¤‘ìš”\s*(?:\)|\]|ã€‘)?[:ï¼š]?\s*(.+)",  # (ì¤‘ìš”) / [ì¤‘ìš”] / ì¤‘ìš”: ...
    r"(?:\(|\[|ã€)?\s*ì£¼ì˜\s*(?:\)|\]|ã€‘)?[:ï¼š]?\s*(.+)",  # (ì£¼ì˜) ...
    r"â€»\s*(.+)",  # â€» ...
    r"(?:í•„ìˆ˜|ì—„ìˆ˜|ê²½ê³ )[:ï¼š]?\s*(.+)",  # í•„ìˆ˜:, ê²½ê³ :
    r"\bMUST\b[:ï¼š]?\s*(.+)",  # MUST: ...
]


def extract_highlights_from_text(text: str, limit=15):
    points = []
    # 1) ë§ˆí¬ë‹¤ìš´ bold ìì²´ë¥¼ ìš”ì ìœ¼ë¡œë„ ì·¨ê¸‰
    for m in re.finditer(r"\*\*(.+?)\*\*", text):
        t = m.group(1).strip()
        if 2 <= len(t) <= 120:  # ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸´ê±´ ì œì™¸
            points.append(("bold", t))

    # 2) ì¤‘ìš”/ì£¼ì˜/â€» ë“±
    for pat in HIGHLIGHT_PATTERNS[1:]:
        for m in re.finditer(pat, text, flags=re.IGNORECASE):
            t = m.group(1).strip() if m.groups() else m.group(0).strip()
            if 2 <= len(t) <= 160:
                points.append(("red", t))

    # ì¤‘ë³µ ì œê±°(ìˆœì„œ ìœ ì§€)
    seen = set()
    uniq = []
    for typ, t in points:
        key = (typ, t)
        if key not in seen:
            seen.add(key)
            uniq.append((typ, t))
        if len(uniq) >= limit:
            break
    return uniq


def collect_batch_highlights(docs, per_doc_limit=6, total_limit=20):
    bag = []
    for d in docs:
        pts = extract_highlights_from_text(d.page_content, limit=per_doc_limit)
        bag.extend(pts)
        if len(bag) >= total_limit:
            break
    # total limit
    return bag[:total_limit]


# -------------------------------
# ğŸ§¾ ìš”ì•½ ìƒì„± (LLM)
# -------------------------------
SUMMARY_PROMPT = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "ë„ˆëŠ” ì—…ë¡œë“œëœ ì‹œë°©ì„œ ë¬¶ìŒì„ í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê³  ì •í™•í•˜ê²Œ ìš”ì•½í•˜ëŠ” ê¸°ìˆ ë¬¸ì„œ ë³´ì¡°ìë‹¤. "
            "ê°€ëŠ¥í•˜ë©´ ì¡°ëª©ì¡°ëª© í•­ëª©í˜•ìœ¼ë¡œ, ìˆ˜ì¹˜/ì¹˜ìˆ˜/ì¬ë£Œ/ì‹œê³µìˆœì„œ/ê²€ìˆ˜ê¸°ì¤€ì„ êµ¬ë¶„í•´ ì •ë¦¬í•˜ë¼. "
            "ì…ë ¥ìœ¼ë¡œ ì „ë‹¬ë˜ëŠ” 'ìš”ì  í›„ë³´'ëŠ” êµµê²Œ ê°•ì¡°í•´ì„œ ìƒë‹¨ì— ë¨¼ì € ì •ë¦¬í•˜ë¼."
            "ì œëª© ë§ˆí¬ë‹¤ìš´ ì™¸ ë³¸ë¬¸ì— ì´ëª¨í‹°ì½˜ì€ ì‚¬ìš©í•˜ì§€ ë§ˆë¼.",
        ),
        (
            "human",
            """ë‹¤ìŒì€ ì´ë²ˆ ë°°ì¹˜ì— í¬í•¨ëœ ë¬¸ì„œë“¤ì˜ ë°œì·Œ í…ìŠ¤íŠ¸ë‹¤.

[ìš”ì  í›„ë³´]
{points}

[ë¬¸ì„œ ë‚´ìš©(ìƒ˜í”Œ)]
{content}

ìš”ì•½/ë³‘í•© ê·œì¹™:
- ë™ì¼ í•­ëª©ì—ì„œ ì„œë¡œ ë‹¤ë¥¸ ê°’ì´ ìˆìœ¼ë©´ **ë¬¸ì„œ ë©”íƒ€ë°ì´í„°ì˜ timestampê°€ ê°€ì¥ ìµœê·¼ì¸ ê°’ë§Œ** ì±„íƒí•œë‹¤.
- v1/v2 ê°™ì€ **ë²„ì „ ë¼ë²¨ì„ ë³¸ë¬¸ì— ì“°ì§€ ë§ë¼**. ê³¼ê±°ê°’ì€ 'ì°¸ê³  ê·¼ê±°'ì—ë§Œ í•„ìš”ì‹œ ìš”ì•½-ë¹„êµí•˜ë¼.
- ì¦‰, ìµœì¢… ë³¸ë¬¸ì€ **ìµœì‹  ê¸°ì¤€ìœ¼ë¡œ ë³‘í•©ëœ ë‹¨ì¼ ì‚¬ì–‘**ë§Œ ì ëŠ”ë‹¤.


ì›í•˜ëŠ” ì¶œë ¥ í˜•ì‹(ë§ˆí¬ë‹¤ìš´):

- ë¬¸ì„œ ëª©ë¡: íŒŒì¼ëª…1, íŒŒì¼ëª…2, ...

### ğŸ”´ ìš”ì 
- **êµµê²Œ í‘œì‹œ** í•­ëª©ìœ¼ë¡œ 5~12ê°œ í•µì‹¬ë§Œ.

---

### ğŸ“Œ ì£¼ìš” ì‚¬ì–‘
- **ì¬ë£Œ:**
- **ì¹˜ìˆ˜/ê·œê²©:**
- **ì‹œê³µ ì ˆì°¨/ìˆœì„œ:**
- **í’ˆì§ˆ/ê²€ìˆ˜/ìœ ì˜:**

---

### ğŸ“ ì°¸ê³  ê·¼ê±°
<details>
  <summary><b>ğŸ” ê·¼ê±° í¼ì¹˜ê¸° / ì ‘ê¸°</b></summary>

- [íŒŒì¼/í˜ì´ì§€] í•µì‹¬ë¬¸ì¥ ìš”ì•½
- [íŒŒì¼/í˜ì´ì§€] í•µì‹¬ë¬¸ì¥ ìš”ì•½
- (í•„ìš” ì‹œ ì¶”ê°€)

</details>

---

### ìš”ì•½
- 1)
- 2)
- 3)

---

ì£¼ì˜: ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ê³  ë¹„ì›Œë‘ê±°ë‚˜ 'í•´ë‹¹ì‚¬í•­ ì—†ìŒ'ìœ¼ë¡œ í‘œê¸°.
""",
        ),
    ]
)


def make_batch_summary(docs, model="gpt-5-mini"):
    # íŒŒì¼ëª… ë¦¬ìŠ¤íŠ¸
    names = []
    for d in docs:
        disp = (
            d.metadata.get("display_name")
            or os.path.basename(d.metadata.get("source", "") or "")
            or "document"
        )
        if disp not in names:
            names.append(disp)
    names_str = ", ".join(names[:12]) + (" ..." if len(names) > 12 else "")

    # ìš”ì  í›„ë³´ ìˆ˜ì§‘
    key_points = collect_batch_highlights(docs, per_doc_limit=6, total_limit=20)

    # ì»¨í…ì¸  ìƒ˜í”Œ(ë„ˆë¬´ ê¸¸ë©´ ì•ë¶€ë¶„ë§Œ)
    samples = []
    for d in docs:
        t = d.page_content.strip().replace("\n\n", "\n")
        if not t:
            continue
        samples.append(t[:700])  # ìƒ˜í”Œ ê¸¸ì´ ì ë‹¹íˆ ì œí•œ
    sample_text = "\n\n---\n\n".join(samples)[:4000]

    # ìš”ì  í›„ë³´ë¥¼ ë§ˆí¬ë‹¤ìš´/HTML ì„ì–´ì„œ ë¯¸ë¦¬ ì •ë¦¬
    pts_lines = []
    for typ, t in key_points:
        pts_lines.append(
            f"- **{t}**" if typ == "bold" else f'- <span class="red-point">{t}</span>'
        )
    pts_block = "\n".join(pts_lines) if pts_lines else "- (ìë™ ì¶”ì¶œëœ ìš”ì  ì—†ìŒ)"

    # âœ… íŒŒì´í”„ ì²´ì¸ìœ¼ë¡œ ì•ˆì „ í˜¸ì¶œ
    llm = ChatOpenAI(model=model)
    summary_chain = SUMMARY_PROMPT | llm
    msg = summary_chain.invoke({"points": pts_block, "content": sample_text})

    rendered_inner = f"<h3>ì´ë²ˆ ë°°ì¹˜ ë¬¸ì„œ:{names_str}</h3>\n\n{msg.content}"
    rendered = f'<div class="summary-card">{rendered_inner}</div>'

    return rendered


# ---------------------------------------
# ì—…ë¡œë”/ì¸ë±ì„œ
# ---------------------------------------
st.subheader("1) ì‹œë°©ì„œ ì—…ë¡œë“œ")
uploaded = st.file_uploader(
    "PDF(.pdf) ë˜ëŠ” í…ìŠ¤íŠ¸(.txt/.md) ì‹œë°©ì„œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš” (ë³µìˆ˜ ê°€ëŠ¥)",
    type=["pdf", "txt", "md"],
    accept_multiple_files=True,
)

col_a, col_b = st.columns(2)
with col_a:
    if st.button("ğŸ“š ì¸ë±ìŠ¤ ìƒì„±", use_container_width=True, type="primary"):
        if not uploaded:
            st.warning("ë¨¼ì € íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.")
        else:
            with st.spinner("ë¬¸ì„œ ë¡œë”©/ì²­í¬ ë¶„í• /ì„ë² ë”© ì¤‘..."):
                # ì´ë²ˆ ì—…ë¡œë“œ ë°°ì¹˜ë§Œ ë³„ë„ë¡œ ë¡œë”©
                raw_docs = load_docs(uploaded)
                chunks = split_docs(
                    raw_docs, chunk_size=chunk_size, chunk_overlap=chunk_overlap
                )

                embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
                vs = FAISS.from_documents(chunks, embeddings)
                st.session_state["vectorstore"] = vs

                # ğŸ”¹ ì´ë²ˆ ë°°ì¹˜ë¥¼ ì €ì¥(ìš”ì•½ì€ 'ì´ë²ˆ ë°°ì¹˜ ìš°ì„ ' ìƒì„±)
                st.session_state["last_index_batch_docs"] = raw_docs

                # ğŸ”¹ ë°”ë¡œ ìš”ì•½ ìƒì„±
                st.session_state["last_index_summary"] = make_batch_summary(
                    raw_docs, model=model_name
                )

            st.success(f"ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ! (ì²­í¬ ìˆ˜: {len(chunks)})")

with col_b:
    if st.button("ğŸ—‘ ì¸ë±ìŠ¤ ì´ˆê¸°í™”", use_container_width=True):
        st.session_state["vectorstore"] = None
        st.session_state["chat_history"] = []
        st.session_state["last_index_batch_docs"] = []
        st.session_state["last_index_summary"] = None
        st.success("ì´ˆê¸°í™” ì™„ë£Œ.")

# ---------------------------------------
# âœ… ëª¨ìˆœ(ì¶©ëŒ) ê°ì§€/ë³‘í•© ê·œì¹™
# ---------------------------------------

# ---- ê°„ë‹¨ ê·œì¹™ ê¸°ë°˜ ì¶”ì¶œê¸° (ìˆ«ì/ë¶€ë“±í˜¸/ë‹¨ìœ„ & ê¸/ë¶€ì • ì„œìˆ )
NUM_PAT = re.compile(
    r"(?P<key>[ê°€-í£A-Za-z0-9\s\-/\(\)Â·]+?)\s*"
    r"(?P<op>â‰¥|<=|â‰¤|>=|=|>|<|â‰ˆ|~)?\s*"
    r"(?P<val>\d+(?:\.\d+)?)\s*"
    r"(?P<unit>mm|cm|m|W|kW|%|EA|MPa|CMH|A|V|mmH2O|dB\(A\))?",
    flags=re.UNICODE,
)
NEG_PAT = re.compile(r"(ê¸ˆì§€|ë¬´|ì•„ë‹˜|ì•„ë‹ˆë‹¤|ì—†ìŒ|ë¶ˆê°€)")
POS_PAT = re.compile(r"(í•„ìˆ˜|í¬í•¨|ì„¤ì¹˜|ì ìš©|í•„ìš”|ìˆìŒ)")


def _normalize_key(raw: str) -> str:
    t = re.sub(r"[\s/()Â·]+", " ", raw).strip().lower()
    # ë„ˆë¬´ ê¸´ í‚¤ëŠ” ì»·
    return t[:120]


def extract_facts(doc) -> list[dict]:
    facts = []
    text = doc.page_content
    for m in NUM_PAT.finditer(text):
        facts.append(
            {
                "type": "numeric",
                "key": _normalize_key(m.group("key")),
                "op": m.group("op") or "=",
                "val": float(m.group("val")),
                "unit": (m.group("unit") or "").lower(),
                "source": doc.metadata.get("display_name", "document"),
                "page": doc.metadata.get("page"),
                "ts": doc.metadata.get("timestamp"),
            }
        )
    # ì„œìˆ í˜• (+/-) ì¡´ì¬ì„±
    for line in text.splitlines():
        line = line.strip()
        if not line:
            continue
        key = _normalize_key(line)
        if POS_PAT.search(line):
            facts.append(
                {
                    "type": "bool",
                    "key": key,
                    "polarity": True,
                    "source": doc.metadata.get("display_name", "document"),
                    "page": doc.metadata.get("page"),
                    "ts": doc.metadata.get("timestamp"),
                }
            )
        if NEG_PAT.search(line):
            facts.append(
                {
                    "type": "bool",
                    "key": key,
                    "polarity": False,
                    "source": doc.metadata.get("display_name", "document"),
                    "page": doc.metadata.get("page"),
                    "ts": doc.metadata.get("timestamp"),
                }
            )
    return facts


def detect_conflicts(docs: list) -> dict:
    """
    ë°˜í™˜:
    {
      "numeric_conflicts": [ {key, entries:[...], merged} ],
      "boolean_conflicts": [ {key, positives:[...], negatives:[...], resolution} ],
      "constraint_violations": [ {rule, evidence:[...]} ]
    }
    ë³‘í•© ê·œì¹™:
      - ìµœì‹ (timestamp í°) ê°’ì„ ìš°ì„ 
      - ë‹¨ìœ„ ë™ì¼ ì‹œ ê°’ì´ ë‹¤ë¥´ë©´ 'ì¶©ëŒ'
      - ë¶€ë“±í˜¸/ì¡°ê±´ ì¶©ëŒë„ í‘œê¸°
    """
    by_key_num = {}
    by_key_bool = {}

    for d in docs:
        for f in extract_facts(d):
            if f["type"] == "numeric":
                by_key_num.setdefault((f["key"], f["unit"]), []).append(f)
            else:
                by_key_bool.setdefault(f["key"], []).append(f)

    numeric_conflicts = []
    for (key, unit), items in by_key_num.items():
        # ì„œë¡œ ë‹¤ë¥¸ ê°’/ì—°ì‚°ìê°€ ì¡´ì¬í•˜ë©´ ì¶©ëŒ í›„ë³´
        vals = {(it["op"], it["val"]) for it in items}
        if len(vals) > 1:
            # ìµœì‹  ìš°ì„  ë³‘í•©ì•ˆ: ê°€ì¥ ìµœì‹  ts
            items_sorted = sorted(items, key=lambda x: (x["ts"] or "",), reverse=True)
            merged = {
                "op": items_sorted[0]["op"],
                "val": items_sorted[0]["val"],
                "unit": unit,
                "ts": items_sorted[0]["ts"],
                "source": items_sorted[0]["source"],
            }
            numeric_conflicts.append(
                {"key": key, "unit": unit, "entries": items_sorted, "merged": merged}
            )

    boolean_conflicts = []
    for key, items in by_key_bool.items():
        pos = [it for it in items if it["polarity"]]
        neg = [it for it in items if not it["polarity"]]
        if pos and neg:
            # ìµœì‹  ìš°ì„ : ë” ìµœì‹  ìª½ ì±„íƒ
            newest_pos_ts = max((p["ts"] or "" for p in pos), default="")
            newest_neg_ts = max((n["ts"] or "" for n in neg), default="")
            resolution = True if newest_pos_ts >= newest_neg_ts else False
            boolean_conflicts.append(
                {
                    "key": key,
                    "positives": pos,
                    "negatives": neg,
                    "resolution": resolution,  # True ì±„íƒ/ False ì±„íƒ
                }
            )

    # ì œì•½ ìœ„ë°˜: ê°„ë‹¨ ê·œì¹™ ì˜ˆ) "A < B"ì¸ë° "= B" ë“±ì¥
    # í…ìŠ¤íŠ¸ ê¸°ë°˜ì´ë¼ í‚¤ ë§¤í•‘ì´ ì–´ë ¤ì›Œ ë³´ìˆ˜ì ìœ¼ë¡œ íƒì§€
    constraint_violations = []
    # ì˜ˆì‹œ ê·œì¹™: ê°™ì€ key/unitì—ì„œ (< ë˜ëŠ” â‰¤) vs (= ë˜ëŠ” >, â‰¥)ê°€ ê³µì¡´í•˜ê³  ê°’ì´ ë™ì¼/ì—­ì „
    for (key, unit), items in by_key_num.items():
        ops = set(it["op"] for it in items)
        if any(op in ops for op in ["<", "â‰¤"]) and any(
            op in ops for op in ["=", ">", "â‰¥"]
        ):
            # ê°„ë‹¨: ê°’ë“¤ì˜ min/maxê°€ ì„œë¡œ ëª¨ìˆœì¸ì§€ ì²´í¬
            vals = [it["val"] for it in items]
            if vals:
                mn, mx = min(vals), max(vals)
                if mn == mx or mn > mx:
                    constraint_violations.append(
                        {
                            "rule": f"{key} ì œì•½ ì¶©ëŒ({unit}): '< or â‰¤' ì™€ '= or > or â‰¥' í˜¼ì¬",
                            "evidence": items,
                        }
                    )

    return {
        "numeric_conflicts": numeric_conflicts,
        "boolean_conflicts": boolean_conflicts,
        "constraint_violations": constraint_violations,
    }


# ---------------------------------------
# âœ… ì—…ë¡œë“œ ì§í›„ ìš”ì•½ë³¸ ì¶œë ¥ (ìƒˆ ì¸ë±ìŠ¤ ìš°ì„ )
# ---------------------------------------
if st.session_state.get("last_index_summary"):
    st.markdown("### ì—…ë¡œë“œ ë°°ì¹˜ ìš”ì•½ë³¸")
    st.markdown(st.session_state["last_index_summary"], unsafe_allow_html=True)

# conflicts = detect_conflicts(st.session_state["last_index_batch_docs"])
# st.session_state["last_batch_conflicts"] = conflicts

# if st.session_state.get("last_batch_conflicts"):
#     cf = st.session_state["last_batch_conflicts"]
#     st.markdown("#### ğŸ§© ë¬¸ì„œ ì¶©ëŒ/ëª¨ìˆœ ê°ì§€ ê²°ê³¼")
#     with st.expander("ğŸ” ìƒì„¸ ë³´ê¸° (ìˆ˜ì¹˜/ì„œìˆ /ì œì•½ ìœ„ë°˜)"):
#         # ìˆ˜ì¹˜í˜•
#         st.markdown("**ìˆ˜ì¹˜í˜• ì¶©ëŒ (numeric)**")
#         if cf["numeric_conflicts"]:
#             for c in cf["numeric_conflicts"]:
#                 st.write(f"- í‚¤: `{c['key']}` [{c['unit'] or '-'}]")
#                 for e in c["entries"]:
#                     page = (e["page"] + 1) if isinstance(e["page"], int) else "N/A"
#                     st.write(
#                         f"   â€¢ {e['source']} p.{page}: {e['op']} {e['val']} {e['unit'] or ''} @ {e['ts']}"
#                     )
#                 m = c["merged"]
#                 st.write(
#                     f"   â†’ **ë³‘í•© ê¶Œê³ (ìµœì‹ ìš°ì„ )**: {m['op']} {m['val']} {m['unit'] or ''} (from {m['source']}, {m['ts']})"
#                 )
#         else:
#             st.write("- ì—†ìŒ")

#         st.markdown("---")
#         # ì„œìˆ í˜•
#         st.markdown("**ì„œìˆ /ë²”ì£¼ ì¶©ëŒ (boolean)**")
#         if cf["boolean_conflicts"]:
#             for c in cf["boolean_conflicts"]:
#                 st.write(f"- í‚¤: `{c['key']}`")
#                 st.write(
#                     "  â€¢ ê¸ì • ê·¼ê±° ìˆ˜: "
#                     + str(len(c["positives"]))
#                     + " / ë¶€ì • ê·¼ê±° ìˆ˜: "
#                     + str(len(c["negatives"]))
#                 )
#                 st.write(
#                     f"  â†’ **ì±„íƒ(ìµœì‹ ìš°ì„ )**: {'ê¸ì •' if c['resolution'] else 'ë¶€ì •'}"
#                 )
#         else:
#             st.write("- ì—†ìŒ")

#         st.markdown("---")
#         # ì œì•½ ìœ„ë°˜
#         st.markdown("**ì œì•½ ìœ„ë°˜ (constraints)**")
#         if cf["constraint_violations"]:
#             for v in cf["constraint_violations"]:
#                 st.write(f"- {v['rule']}")
#                 for e in v["evidence"]:
#                     page = (e["page"] + 1) if isinstance(e["page"], int) else "N/A"
#                     st.write(
#                         f"   â€¢ {e['source']} p.{page}: {e['op']} {e['val']} {e['unit'] or ''} @ {e['ts']}"
#                     )
#         else:
#             st.write("- ì—†ìŒ")


# ---------------------------------------
# RAG ì²´ì¸ êµ¬ì„±
# ---------------------------------------
def make_rag_chain(vectorstore):
    retriever = vectorstore.as_retriever(
        search_type="mmr", search_kwargs={"k": k_ctx, "fetch_k": max(10, k_ctx * 4)}
    )
    llm = ChatOpenAI(model=model_name)

    def format_docs(docs):
        formatted = []
        for d in docs:
            src_path = d.metadata.get("source", "")
            page = d.metadata.get("page", None)
            disp = d.metadata.get(
                "display_name", os.path.basename(src_path) or "document"
            )
            head = f"[source: {disp}"
            if page is not None:
                head += f", page: {page+1}"
            head += "]"
            formatted.append(f"{head}\n{d.page_content}")
        return "\n\n---\n\n".join(formatted)

    rag = (
        {
            "context": (lambda x: x["question"]) | retriever | format_docs,
            "question": lambda x: x["question"],
            "chat_history": lambda x: x["chat_history"],
        }
        | USER_PROMPT
        | llm
    )
    return rag, retriever


# ---------------------------------------
# ì§ˆì˜ ì˜ì—­
# ---------------------------------------
st.subheader("2) ì§ˆë¬¸í•˜ê¸°")
q = st.text_input(
    "ì‹œë°©ì„œ ê´€ë ¨ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: 'UBR ê³µì‚¬ì—ì„œ ë²½ì²´ íƒ€ì¼ ê·œê²©ì€?')"
)

if st.session_state["vectorstore"] is None:
    st.info("ë¨¼ì € ì‹œë°©ì„œë¥¼ ì—…ë¡œë“œí•˜ê³  ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•˜ì„¸ìš”.")
else:
    rag_chain, retriever = make_rag_chain(st.session_state["vectorstore"])

    if st.button("ğŸ” ì§ˆì˜ ì‹¤í–‰", type="primary") and q.strip():
        with st.spinner("ê²€ìƒ‰ ë° ë‹µë³€ ìƒì„± ì¤‘..."):
            docs = search_with_recency_rerank(
                st.session_state["vectorstore"],
                q,
                k=k_ctx,
                fetch_k=max(24, k_ctx * 6),
                w_recency=0.35,
                half_life_days=14,
            )
            chat_history_str = (
                "\n".join(
                    [
                        f"Q: {qq}\nA: {aa}"
                        for qq, aa in st.session_state["chat_history"]
                    ][-6:]
                )
                if st.session_state["chat_history"]
                else "ì—†ìŒ"
            )

            answer_msg = rag_chain.invoke(
                {"question": q, "chat_history": chat_history_str}
            )

        st.session_state["chat_history"].append((q, answer_msg.content))

        st.markdown("### ğŸ§  ë‹µë³€")
        st.markdown(answer_msg.content)

        with st.expander("ğŸ” ì‚¬ìš©í•œ ê·¼ê±°(ìƒìœ„ ê²€ìƒ‰ ê²°ê³¼) ë³´ê¸°"):
            for i, d in enumerate(docs, 1):
                src_path = d.metadata.get("source", "")
                page = d.metadata.get("page", None)
                disp = d.metadata.get(
                    "display_name", os.path.basename(src_path) or "document"
                )
                st.markdown(
                    f"**[{i}] {disp}**  (page: {page+1 if page is not None else 'N/A'})"
                )
                st.write(
                    d.page_content[:1200]
                    + ("..." if len(d.page_content) > 1200 else "")
                )

# ---------------------------------------
# íˆìŠ¤í† ë¦¬ í‘œì‹œ
# ---------------------------------------
if st.session_state["chat_history"]:
    st.markdown("---")
    st.markdown("### ğŸ’¬ ëŒ€í™” íˆìŠ¤í† ë¦¬")
    for i, (qq, aa) in enumerate(reversed(st.session_state["chat_history"][-8:]), 1):
        st.markdown(f"**Q{i}.** {qq}")
        st.markdown(f"**A{i}.** {aa}")
