import os
import tempfile
import shutil
import re
import streamlit as st
from dotenv import load_dotenv
from datetime import datetime, timezone
from zoneinfo import ZoneInfo

SEOUL_TZ = ZoneInfo("Asia/Seoul")

# LangChain (ìµœì‹  êµ¬ì¡°)
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import PyPDFLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough


def _sidebar_dark_and_slider_fix():
    st.markdown(
        """
    <style>
      :root{
        /* Sidebar dark palette */
        --sb-bg:#0b1220;
        --sb-fg:#e2e8f0;
        --sb-muted:#e5e7eb;
        --sb-line:#1f2a44;

        --accent:#f1f5f9;
        --accent-2:#cbd5e1;

        /* Main content neutrals */
        --ink:#0f172a;
        --muted:#475569;
        --line:#e2e8f0;
      }

      /* Sidebar Dark */
      section[data-testid="stSidebar"]{
        background:var(--sb-bg)!important; color:var(--sb-fg)!important;
        border-right:1px solid var(--sb-line);
      }
      section[data-testid="stSidebar"] *{ color:var(--sb-fg)!important; }
      section[data-testid="stSidebar"] h1,section[data-testid="stSidebar"] h2,section[data-testid="stSidebar"] h3{
        color:var(--sb-fg)!important;
      }

      /* helper labels */
      section[data-testid="stSidebar"] .stMarkdown p,
      section[data-testid="stSidebar"] label,
      section[data-testid="stSidebar"] .stSelectbox label{
        color:var(--sb-muted)!important;
        font-weight:600!important;
      }

      /* Inputs in sidebar */
      section[data-testid="stSidebar"] input,
      section[data-testid="stSidebar"] textarea,
      section[data-testid="stSidebar"] select,
      section[data-testid="stSidebar"] .stTextInput input,
      section[data-testid="stSidebar"] .stNumberInput input{
        background:rgba(255,255,255,0.06)!important;
        border:1px solid var(--sb-line)!important;
      }

      /* Slider cutoff fix */
      section[data-testid="stSidebar"] [data-testid="stVerticalBlock"]{ padding-right:12px; }
      section[data-testid="stSidebar"] div[data-testid="stSlider"]{
        padding-right:12px; margin-right:2px; overflow:visible;
      }
      section[data-testid="stSidebar"] div[role="slider"]{
        box-shadow:0 0 0 1px rgba(20,184,166,0.25); border-radius:999px;
      }

      input[type="radio"]{ accent-color: var(--accent); }
      div[role="radiogroup"] label{
        display:flex; align-items:center; gap:.5rem;
        line-height:1.2; margin: .1rem 0;
      }

      section[data-testid="stSidebar"] .stButton>button,
      [data-testid="stAppViewContainer"] .stButton>button{
        background:linear-gradient(180deg,var(--accent),var(--accent-2))!important;
        color:#001018!important;
        border:0!important; font-weight:800!important; letter-spacing:.2px;
        border-radius:10px; padding:.55rem 1rem;
      }
      section[data-testid="stSidebar"] .stButton>button:hover,
      [data-testid="stAppViewContainer"] .stButton>button:hover{
        filter:brightness(1.05);
      }

      [data-testid="stImage"]{ margin:6px 0 18px!important; }
      [data-testid="stImage"] img{ display:block; }

      /* ì‚¬ì´ë“œë°” í˜ì´ì§€ ë¼ë²¨ ë°”ê¾¸ê¸° (ì˜ˆì‹œ) */
      span[label="app main"] { font-size:0 !important; position:relative; }
      span[label="app main"]::after {
        content:"ë©”ì¸"; font-size:1rem !important; color:#fff !important; font-weight:700 !important;
        position:absolute; left:0; top:0;
      }
    </style>
    """,
        unsafe_allow_html=True,
    )


# call once
_sidebar_dark_and_slider_fix()

# ìš”ì•½ ì¹´ë“œ ê³µí†µ ìŠ¤íƒ€ì¼ (ë‘¥ê·¼ ëª¨ì„œë¦¬ + ê·¸ë¦¼ì)
st.markdown(
    """
<style>
  .summary-card{
    border:1px solid var(--line);
    border-radius:14px;
    padding:16px 20px;
    background:#ffffff;
    margin-top:.5rem;
    margin-bottom:3.5rem;
  }
  /* ì¹´ë“œ ë‚´ë¶€ elements ì‚´ì§ ì •ëˆ */
  .summary-card h1, .summary-card h2, .summary-card h3{ margin-top:.6rem; }
  .summary-card hr{ border:none; border-top:1px solid #e5e7eb; margin:12px 0; }
  .summary-card details{
    margin-top:.5rem;
    background:#f8fafc;
    border:1px solid #e2e8f0;
    border-radius:10px;
    padding:.5rem .75rem;
  }
  .summary-card summary{ cursor:pointer; font-weight:700; }
</style>
""",
    unsafe_allow_html=True,
)

# ---------------------------------------
# í™˜ê²½ì„¤ì •
# ---------------------------------------
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
assert OPENAI_API_KEY, "OPENAI_API_KEYê°€ .envì— ì—†ìŠµë‹ˆë‹¤."

st.set_page_config(page_title="ì‹œë°©ì„œ Q&A ì±—ë´‡", page_icon="ğŸ›", layout="wide")
st.title("ğŸ› ì‹œë°©ì„œ Q&A ì±—ë´‡")

# ---------------------------------------
# âœ… ìƒíƒœ ì´ˆê¸°í™”
# ---------------------------------------
if "vectorstore" not in st.session_state:
    st.session_state["vectorstore"] = None
if "chat_history" not in st.session_state:
    st.session_state["chat_history"] = []
# ìƒˆë¡œ ì¶”ê°€: ë§ˆì§€ë§‰ ì¸ë±ìŠ¤ ë°°ì¹˜ì™€ ê·¸ ìš”ì•½
if "last_index_batch_docs" not in st.session_state:
    st.session_state["last_index_batch_docs"] = []
if "last_index_summary" not in st.session_state:
    st.session_state["last_index_summary"] = None

# ---------------------------------------
# ì‚¬ì´ë“œë°”: ëª¨ë¸/ì˜µì…˜
# ---------------------------------------
with st.sidebar:
    st.markdown("### âš™ï¸ ì˜µì…˜")
    model_name = "gpt-5"
    st.markdown("âš™ï¸ LLM ëª¨ë¸: gpt-5")
    k_ctx = st.slider("ê²€ìƒ‰ ë¬¸ì„œ ìˆ˜(k)", 2, 8, 4, 1)
    chunk_size = st.slider("ì²­í¬ í¬ê¸°", 500, 2000, 1000, 100)
    chunk_overlap = st.slider("ì˜¤ë²„ë©", 50, 400, 150, 25)
    st.markdown("---")
    st.markdown("**íŒŒì¼ ì—…ë¡œë“œ í›„, [ì¸ë±ìŠ¤ ìƒì„±]ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.**")


# ---------------------------------------
# ê³µìš©: ì—…ë¡œë“œ íŒŒì¼ì„ ì„ì‹œê²½ë¡œë¡œ ì €ì¥
# ---------------------------------------
def _save_uploaded_to_temp(uploaded_file, suffix):
    """Streamlit UploadedFile -> temp file path"""
    tmp = tempfile.NamedTemporaryFile(delete=False, suffix=suffix)
    try:
        shutil.copyfileobj(uploaded_file, tmp)
        tmp.flush()
        return tmp.name
    finally:
        tmp.close()


# ---------------------------------------
# í•¨ìˆ˜: ë¬¸ì„œ ë¡œë”© (PDF/Text)
# ---------------------------------------
def load_docs(uploaded_files):
    docs = []
    batch_id = datetime.now(tz=SEOUL_TZ).strftime("%Y%m%d-%H%M%S")
    batch_ts = datetime.now(tz=SEOUL_TZ).isoformat()

    for f in uploaded_files:
        suffix = os.path.splitext(f.name)[1].lower()

        if suffix == ".pdf":
            tmp_path = _save_uploaded_to_temp(f, ".pdf")
            try:
                loader = PyPDFLoader(tmp_path)
                loaded = loader.load()
                for d in loaded:
                    d.metadata["display_name"] = f.name
                docs.extend(loaded)
            finally:
                os.unlink(tmp_path)

        elif suffix in [".txt", ".md"]:
            tmp_path = _save_uploaded_to_temp(f, suffix)
            try:
                loader = TextLoader(tmp_path, encoding="utf-8")
                loaded = loader.load()
                for d in loaded:
                    d.metadata["display_name"] = f.name
                    d.metadata["batch_id"] = batch_id
                    d.metadata["timestamp"] = batch_ts
                docs.extend(loaded)
            finally:
                os.unlink(tmp_path)
        else:
            st.warning(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” í˜•ì‹: {f.name}")

    return docs


# ---------------------------------------
# í•¨ìˆ˜: ì²­í¬ ë¶„í• 
# ---------------------------------------
def split_docs(docs, chunk_size=1000, chunk_overlap=150):
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        separators=["\n\n", "\n", " ", ""],
    )
    return splitter.split_documents(docs)


# ---------------------------------------
# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ìš•ì‹¤ ê³µì‚¬ ì‹œë°©ì„œ ì „ìš©)
# ---------------------------------------
SYSTEM_INSTRUCTIONS = """\
ë„ˆëŠ” ìš•ì‹¤(UBR) ê³µì‚¬ ì‹œë°©ì„œ ì „ìš© ì „ë¬¸ê°€ ì–´ì‹œìŠ¤í„´íŠ¸ë‹¤.
- ë°˜ë“œì‹œ ì—…ë¡œë“œëœ ì‹œë°©ì„œ/ë„ë©´(ì»¨í…ìŠ¤íŠ¸)ì— ê·¼ê±°í•´ ëŒ€ë‹µí•˜ë¼.
- ê·¼ê±°ê°€ ë¶ˆì¶©ë¶„í•˜ë©´ 'í•´ë‹¹ì‚¬í•­ ì—†ìŒ' ë˜ëŠ” 'ì‹œë°©ì„œì— ëª…ì‹œ ì—†ìŒ'ì´ë¼ê³  ë‹µí•˜ê³  ì¶”ì¸¡í•˜ì§€ ë§ˆë¼.
- ì§ˆë¬¸ì´ ì‹œë°©ì„œ ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ë©´ 'ë³¸ ì±—ë´‡ì€ ì‹œë°©ì„œ ê¸°ë°˜ ì§ˆì˜ë§Œ ë‹µë³€í•©ë‹ˆë‹¤'ë¼ê³  ì•ˆë‚´í•˜ë¼.
- ìˆ˜ëŸ‰ì´ë‚˜ ì¹˜ìˆ˜ ê³„ì‚°ì´ í•„ìš”í•œ ê²½ìš°, ë¬¸ì„œ ê·¼ê±°(í˜ì´ì§€/ë¬¸êµ¬)ë¥¼ ìš”ì•½í•´ì„œ í•¨ê»˜ ì œì‹œí•˜ë¼.
- ë‹µë³€ì€ í•œêµ­ì–´ë¡œ, í•­ëª©í˜•/í‘œí˜• ì •ë¦¬ ì„ í˜¸.
"""

USER_PROMPT = ChatPromptTemplate.from_messages(
    [
        ("system", SYSTEM_INSTRUCTIONS),
        (
            "human",
            """\
ë‹¤ìŒì€ ê²€ìƒ‰ëœ ì‹œë°©ì„œ ì»¨í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤. ì´ë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µí•˜ë¼.

[ì»¨í…ìŠ¤íŠ¸]
{context}

[ëŒ€í™” íˆìŠ¤í† ë¦¬ ìš”ì•½]
{chat_history}

[ì§ˆë¬¸]
{question}

ìš”êµ¬ì‚¬í•­:
- ë¬¸ì„œ ê·¼ê±°ì˜ í•µì‹¬ ë¬¸êµ¬ë¥¼ ì¸ìš©(ìš”ì•½)í•˜ê³ , ê°€ëŠ¥í•œ ê²½ìš° í˜ì´ì§€/ì„¹ì…˜ì„ í•¨ê»˜ ì œì‹œ.
- ëª¨í˜¸í•˜ë©´ ëª…ì‹œì ìœ¼ë¡œ 'í•´ë‹¹ì‚¬í•­ ì—†ìŒ' ê¸°ì¬.
- ìµœì¢…ì— 'ìš”ì•½' ì„¹ì…˜ìœ¼ë¡œ 3ì¤„ ì´ë‚´ í•µì‹¬ë§Œ ì¬ì •ë¦¬.
""",
        ),
    ]
)

# -------------------------------
# ğŸ”´ ìš”ì (ë³¼ë“œ/ê²½ê³ ) ì¶”ì¶œ ìœ í‹¸
# -------------------------------
HIGHLIGHT_PATTERNS = [
    r"\*\*(.+?)\*\*",  # **bold**
    r"(?:\(|\[|ã€)?\s*ì¤‘ìš”\s*(?:\)|\]|ã€‘)?[:ï¼š]?\s*(.+)",  # (ì¤‘ìš”) / [ì¤‘ìš”] / ì¤‘ìš”: ...
    r"(?:\(|\[|ã€)?\s*ì£¼ì˜\s*(?:\)|\]|ã€‘)?[:ï¼š]?\s*(.+)",  # (ì£¼ì˜) ...
    r"â€»\s*(.+)",  # â€» ...
    r"(?:í•„ìˆ˜|ì—„ìˆ˜|ê²½ê³ )[:ï¼š]?\s*(.+)",  # í•„ìˆ˜:, ê²½ê³ :
    r"\bMUST\b[:ï¼š]?\s*(.+)",  # MUST: ...
]


def extract_highlights_from_text(text: str, limit=15):
    points = []
    # 1) ë§ˆí¬ë‹¤ìš´ bold ìì²´ë¥¼ ìš”ì ìœ¼ë¡œë„ ì·¨ê¸‰
    for m in re.finditer(r"\*\*(.+?)\*\*", text):
        t = m.group(1).strip()
        if 2 <= len(t) <= 120:  # ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸´ê±´ ì œì™¸
            points.append(("bold", t))

    # 2) ì¤‘ìš”/ì£¼ì˜/â€» ë“±
    for pat in HIGHLIGHT_PATTERNS[1:]:
        for m in re.finditer(pat, text, flags=re.IGNORECASE):
            t = m.group(1).strip() if m.groups() else m.group(0).strip()
            if 2 <= len(t) <= 160:
                points.append(("red", t))

    # ì¤‘ë³µ ì œê±°(ìˆœì„œ ìœ ì§€)
    seen = set()
    uniq = []
    for typ, t in points:
        key = (typ, t)
        if key not in seen:
            seen.add(key)
            uniq.append((typ, t))
        if len(uniq) >= limit:
            break
    return uniq


def collect_batch_highlights(docs, per_doc_limit=6, total_limit=20):
    bag = []
    for d in docs:
        pts = extract_highlights_from_text(d.page_content, limit=per_doc_limit)
        bag.extend(pts)
        if len(bag) >= total_limit:
            break
    # total limit
    return bag[:total_limit]


# -------------------------------
# ğŸ§¾ ìš”ì•½ ìƒì„± (LLM)
# -------------------------------
SUMMARY_PROMPT = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "ë„ˆëŠ” ì—…ë¡œë“œëœ ì‹œë°©ì„œ ë¬¶ìŒì„ í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê³  ì •í™•í•˜ê²Œ ìš”ì•½í•˜ëŠ” ê¸°ìˆ ë¬¸ì„œ ë³´ì¡°ìë‹¤. "
            "ê°€ëŠ¥í•˜ë©´ ì¡°ëª©ì¡°ëª© í•­ëª©í˜•ìœ¼ë¡œ, ìˆ˜ì¹˜/ì¹˜ìˆ˜/ì¬ë£Œ/ì‹œê³µìˆœì„œ/ê²€ìˆ˜ê¸°ì¤€ì„ êµ¬ë¶„í•´ ì •ë¦¬í•˜ë¼. "
            "ì…ë ¥ìœ¼ë¡œ ì „ë‹¬ë˜ëŠ” 'ìš”ì  í›„ë³´'ëŠ” êµµê²Œ ê°•ì¡°í•´ì„œ ìƒë‹¨ì— ë¨¼ì € ì •ë¦¬í•˜ë¼."
            "ì œëª© ë§ˆí¬ë‹¤ìš´ ì™¸ ë³¸ë¬¸ì— ì´ëª¨í‹°ì½˜ì€ ì‚¬ìš©í•˜ì§€ ë§ˆë¼.",
        ),
        (
            "human",
            """ë‹¤ìŒì€ ì´ë²ˆ ë°°ì¹˜ì— í¬í•¨ëœ ë¬¸ì„œë“¤ì˜ ë°œì·Œ í…ìŠ¤íŠ¸ë‹¤.

[ìš”ì  í›„ë³´]
{points}

[ë¬¸ì„œ ë‚´ìš©(ìƒ˜í”Œ)]
{content}

ì›í•˜ëŠ” ì¶œë ¥ í˜•ì‹(ë§ˆí¬ë‹¤ìš´):

- ë¬¸ì„œ ëª©ë¡: íŒŒì¼ëª…1, íŒŒì¼ëª…2, ...

### ğŸ”´ ìš”ì 
- **êµµê²Œ í‘œì‹œ** í•­ëª©ìœ¼ë¡œ 5~12ê°œ í•µì‹¬ë§Œ.

---

### ğŸ“Œ ì£¼ìš” ì‚¬ì–‘
- <strong>ì¬ë£Œ</strong>:
- <strong>ì¹˜ìˆ˜/ê·œê²©</strong>:
- <strong>ì‹œê³µ ì ˆì°¨/ìˆœì„œ</strong>:
- <strong>í’ˆì§ˆ/ê²€ìˆ˜/ìœ ì˜</strong>:

---

### ğŸ“ ì°¸ê³  ê·¼ê±°
<details>
  <summary><b>ğŸ” ê·¼ê±° í¼ì¹˜ê¸° / ì ‘ê¸°</b></summary>

- [íŒŒì¼/í˜ì´ì§€] í•µì‹¬ë¬¸ì¥ ìš”ì•½
- [íŒŒì¼/í˜ì´ì§€] í•µì‹¬ë¬¸ì¥ ìš”ì•½
- (í•„ìš” ì‹œ ì¶”ê°€)

</details>

---

### ìš”ì•½
- 1)
- 2)
- 3)

---

ì£¼ì˜: ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ê³  ë¹„ì›Œë‘ê±°ë‚˜ 'í•´ë‹¹ì‚¬í•­ ì—†ìŒ'ìœ¼ë¡œ í‘œê¸°.
""",
        ),
    ]
)


def make_batch_summary(docs, model="gpt-5"):
    # íŒŒì¼ëª… ë¦¬ìŠ¤íŠ¸
    names = []
    for d in docs:
        disp = (
            d.metadata.get("display_name")
            or os.path.basename(d.metadata.get("source", "") or "")
            or "document"
        )
        if disp not in names:
            names.append(disp)
    names_str = ", ".join(names[:12]) + (" ..." if len(names) > 12 else "")

    # ìš”ì  í›„ë³´ ìˆ˜ì§‘
    key_points = collect_batch_highlights(docs, per_doc_limit=6, total_limit=20)

    # ì»¨í…ì¸  ìƒ˜í”Œ(ë„ˆë¬´ ê¸¸ë©´ ì•ë¶€ë¶„ë§Œ)
    samples = []
    for d in docs:
        t = d.page_content.strip().replace("\n\n", "\n")
        if not t:
            continue
        samples.append(t[:700])  # ìƒ˜í”Œ ê¸¸ì´ ì ë‹¹íˆ ì œí•œ
    sample_text = "\n\n---\n\n".join(samples)[:4000]

    # ìš”ì  í›„ë³´ë¥¼ ë§ˆí¬ë‹¤ìš´/HTML ì„ì–´ì„œ ë¯¸ë¦¬ ì •ë¦¬
    pts_lines = []
    for typ, t in key_points:
        pts_lines.append(
            f"- **{t}**" if typ == "bold" else f'- <span class="red-point">{t}</span>'
        )
    pts_block = "\n".join(pts_lines) if pts_lines else "- (ìë™ ì¶”ì¶œëœ ìš”ì  ì—†ìŒ)"

    # âœ… íŒŒì´í”„ ì²´ì¸ìœ¼ë¡œ ì•ˆì „ í˜¸ì¶œ
    llm = ChatOpenAI(model=model)
    summary_chain = SUMMARY_PROMPT | llm
    msg = summary_chain.invoke({"points": pts_block, "content": sample_text})

    rendered_inner = f"<h3>ì´ë²ˆ ë°°ì¹˜ ë¬¸ì„œ:{names_str}</h3>\n\n{msg.content}"
    rendered = f'<div class="summary-card">{rendered_inner}</div>'

    return rendered


# ---------------------------------------
# ì—…ë¡œë”/ì¸ë±ì„œ
# ---------------------------------------
st.subheader("1) ì‹œë°©ì„œ ì—…ë¡œë“œ")
uploaded = st.file_uploader(
    "PDF(.pdf) ë˜ëŠ” í…ìŠ¤íŠ¸(.txt/.md) ì‹œë°©ì„œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš” (ë³µìˆ˜ ê°€ëŠ¥)",
    type=["pdf", "txt", "md"],
    accept_multiple_files=True,
)

col_a, col_b = st.columns(2)
with col_a:
    if st.button("ğŸ“š ì¸ë±ìŠ¤ ìƒì„±", use_container_width=True, type="primary"):
        if not uploaded:
            st.warning("ë¨¼ì € íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.")
        else:
            with st.spinner("ë¬¸ì„œ ë¡œë”©/ì²­í¬ ë¶„í• /ì„ë² ë”© ì¤‘..."):
                # ì´ë²ˆ ì—…ë¡œë“œ ë°°ì¹˜ë§Œ ë³„ë„ë¡œ ë¡œë”©
                raw_docs = load_docs(uploaded)
                chunks = split_docs(
                    raw_docs, chunk_size=chunk_size, chunk_overlap=chunk_overlap
                )

                embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
                vs = FAISS.from_documents(chunks, embeddings)
                st.session_state["vectorstore"] = vs

                # ğŸ”¹ ì´ë²ˆ ë°°ì¹˜ë¥¼ ì €ì¥(ìš”ì•½ì€ 'ì´ë²ˆ ë°°ì¹˜ ìš°ì„ ' ìƒì„±)
                st.session_state["last_index_batch_docs"] = raw_docs

                # ğŸ”¹ ë°”ë¡œ ìš”ì•½ ìƒì„±
                st.session_state["last_index_summary"] = make_batch_summary(
                    raw_docs, model=model_name
                )

            st.success(f"ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ! (ì²­í¬ ìˆ˜: {len(chunks)})")

with col_b:
    if st.button("ğŸ—‘ ì¸ë±ìŠ¤ ì´ˆê¸°í™”", use_container_width=True):
        st.session_state["vectorstore"] = None
        st.session_state["chat_history"] = []
        st.session_state["last_index_batch_docs"] = []
        st.session_state["last_index_summary"] = None
        st.success("ì´ˆê¸°í™” ì™„ë£Œ.")

# ---------------------------------------
# âœ… ì—…ë¡œë“œ ì§í›„ ìš”ì•½ë³¸ ì¶œë ¥ (ìƒˆ ì¸ë±ìŠ¤ ìš°ì„ )
# ---------------------------------------
if st.session_state.get("last_index_summary"):
    st.markdown("### ì—…ë¡œë“œ ë°°ì¹˜ ìš”ì•½ë³¸")
    st.markdown(st.session_state["last_index_summary"], unsafe_allow_html=True)
    # í•„ìš”ì‹œ ì¬ìƒì„±(ì˜µì…˜ ë°”ê¾¼ í›„)
    # if st.button("ğŸ” ìš”ì•½ ë‹¤ì‹œ ìƒì„±", help="ì´ë²ˆ ì—…ë¡œë“œ ë°°ì¹˜ ë‚´ìš©ì„ ê¸°ì¤€ìœ¼ë¡œ ì¬ìš”ì•½"):
    #     with st.spinner("ìš”ì•½ ì¬ìƒì„± ì¤‘..."):
    #         st.session_state["last_index_summary"] = make_batch_summary(
    #             st.session_state.get("last_index_batch_docs", []),
    #             model=model_name,
    #         )
    #     st.success("ìš”ì•½ì„ ê°±ì‹ í–ˆìŠµë‹ˆë‹¤.")
    #     st.markdown(st.session_state["last_index_summary"], unsafe_allow_html=True)


# ---------------------------------------
# RAG ì²´ì¸ êµ¬ì„±
# ---------------------------------------
def make_rag_chain(vectorstore):
    retriever = vectorstore.as_retriever(
        search_type="mmr", search_kwargs={"k": k_ctx, "fetch_k": max(10, k_ctx * 4)}
    )
    llm = ChatOpenAI(model=model_name)

    def format_docs(docs):
        formatted = []
        for d in docs:
            src_path = d.metadata.get("source", "")
            page = d.metadata.get("page", None)
            disp = d.metadata.get(
                "display_name", os.path.basename(src_path) or "document"
            )
            head = f"[source: {disp}"
            if page is not None:
                head += f", page: {page+1}"
            head += "]"
            formatted.append(f"{head}\n{d.page_content}")
        return "\n\n---\n\n".join(formatted)

    rag = (
        {
            "context": (lambda x: x["question"]) | retriever | format_docs,
            "question": lambda x: x["question"],
            "chat_history": lambda x: x["chat_history"],
        }
        | USER_PROMPT
        | llm
    )
    return rag, retriever


# ---------------------------------------
# ì§ˆì˜ ì˜ì—­
# ---------------------------------------
st.subheader("2) ì§ˆë¬¸í•˜ê¸°")
q = st.text_input(
    "ì‹œë°©ì„œ ê´€ë ¨ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: 'UBR ê³µì‚¬ì—ì„œ ë²½ì²´ íƒ€ì¼ ê·œê²©ì€?')"
)

if st.session_state["vectorstore"] is None:
    st.info("ë¨¼ì € ì‹œë°©ì„œë¥¼ ì—…ë¡œë“œí•˜ê³  ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•˜ì„¸ìš”.")
else:
    rag_chain, retriever = make_rag_chain(st.session_state["vectorstore"])

    if st.button("ğŸ” ì§ˆì˜ ì‹¤í–‰", type="primary") and q.strip():
        with st.spinner("ê²€ìƒ‰ ë° ë‹µë³€ ìƒì„± ì¤‘..."):
            docs = retriever.invoke(q)

            chat_history_str = (
                "\n".join(
                    [
                        f"Q: {qq}\nA: {aa}"
                        for qq, aa in st.session_state["chat_history"]
                    ][-6:]
                )
                if st.session_state["chat_history"]
                else "ì—†ìŒ"
            )

            answer_msg = rag_chain.invoke(
                {"question": q, "chat_history": chat_history_str}
            )

        st.session_state["chat_history"].append((q, answer_msg.content))

        st.markdown("### ğŸ§  ë‹µë³€")
        st.markdown(answer_msg.content)

        with st.expander("ğŸ” ì‚¬ìš©í•œ ê·¼ê±°(ìƒìœ„ ê²€ìƒ‰ ê²°ê³¼) ë³´ê¸°"):
            for i, d in enumerate(docs, 1):
                src_path = d.metadata.get("source", "")
                page = d.metadata.get("page", None)
                disp = d.metadata.get(
                    "display_name", os.path.basename(src_path) or "document"
                )
                st.markdown(
                    f"**[{i}] {disp}**  (page: {page+1 if page is not None else 'N/A'})"
                )
                st.write(
                    d.page_content[:1200]
                    + ("..." if len(d.page_content) > 1200 else "")
                )

# ---------------------------------------
# íˆìŠ¤í† ë¦¬ í‘œì‹œ
# ---------------------------------------
if st.session_state["chat_history"]:
    st.markdown("---")
    st.markdown("### ğŸ’¬ ëŒ€í™” íˆìŠ¤í† ë¦¬")
    for i, (qq, aa) in enumerate(reversed(st.session_state["chat_history"][-8:]), 1):
        st.markdown(f"**Q{i}.** {qq}")
        st.markdown(f"**A{i}.** {aa}")
