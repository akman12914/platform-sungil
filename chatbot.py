import os
import tempfile
import shutil
import streamlit as st
from dotenv import load_dotenv

# LangChain (ìµœì‹  êµ¬ì¡°)
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import PyPDFLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough


def _sidebar_dark_and_slider_fix():
    st.markdown(
        """
    <style>
      :root{
        --sb-bg:#0b1220; --sb-fg:#e2e8f0; --sb-muted:#cbd5e1; --sb-line:#1f2a44;
        --accent:#22d3ee; --accent-2:#06b6d4;
      }
      /* Sidebar dark */
      section[data-testid="stSidebar"]{
        background:var(--sb-bg)!important; color:var(--sb-fg)!important;
        border-right:1px solid var(--sb-line);
      }
      section[data-testid="stSidebar"] *{ color:var(--sb-fg)!important; }
      section[data-testid="stSidebar"] h1,section[data-testid="stSidebar"] h2,section[data-testid="stSidebar"] h3{
        color:var(--sb-fg)!important;
      }
      /* ë³´ì¡° í…ìŠ¤íŠ¸(ë¼ë²¨) ë” ì„ ëª… + êµµê²Œ */
      section[data-testid="stSidebar"] .stMarkdown p,
      section[data-testid="stSidebar"] label,
      section[data-testid="stSidebar"] .stSelectbox label{
        color:var(--sb-muted)!important; font-weight:600!important;
      }
      /* Inputs */
      section[data-testid="stSidebar"] input, section[data-testid="stSidebar"] textarea,
      section[data-testid="stSidebar"] select,
      section[data-testid="stSidebar"] .stTextInput input,
      section[data-testid="stSidebar"] .stNumberInput input{
        background:rgba(255,255,255,0.06)!important; border:1px solid var(--sb-line)!important;
      }
      /* Sidebar buttons */
      section[data-testid="stSidebar"] .stButton>button{
        background:linear-gradient(180deg,var(--accent),var(--accent-2))!important;
        color:#001018!important; border:0!important; font-weight:800!important; letter-spacing:.2px;
      }
      section[data-testid="stSidebar"] .stButton>button:hover{ filter:brightness(1.05); }

      /* Slider cutoff fix */
      section[data-testid="stSidebar"] [data-testid="stVerticalBlock"]{ padding-right:12px; }
      section[data-testid="stSidebar"] div[data-testid="stSlider"]{
        padding-right:12px; margin-right:2px; overflow:visible;
      }
      section[data-testid="stSidebar"] div[role="slider"]{
        box-shadow:0 0 0 2px rgba(34,211,238,0.25); border-radius:999px;
      }

      /* Main buttons also themed */
      [data-testid="stAppViewContainer"] .stButton>button{
        background:linear-gradient(180deg,var(--accent),var(--accent-2))!important;
        color:#001018!important; border:0!important; font-weight:800!important; letter-spacing:.2px;
      }
      [data-testid="stAppViewContainer"] .stButton>button:hover{ filter:brightness(1.05); }

      /* Images breathing room (ê²¹ì¹¨ ë°©ì§€) */
      [data-testid="stImage"]{ margin:6px 0 18px!important; }
      [data-testid="stImage"] img{ display:block; }
    </style>
    """,
        unsafe_allow_html=True,
    )


# call once
_sidebar_dark_and_slider_fix()

# ---------------------------------------
# í™˜ê²½ì„¤ì •
# ---------------------------------------
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
assert OPENAI_API_KEY, "OPENAI_API_KEYê°€ .envì— ì—†ìŠµë‹ˆë‹¤."

st.set_page_config(page_title="ì‹œë°©ì„œ Q&A ì±—ë´‡", page_icon="ðŸ›", layout="wide")
st.title("ðŸ› ì‹œë°©ì„œ Q&A ì±—ë´‡")

# ---------------------------------------
# âœ… ìƒíƒœ ì´ˆê¸°í™” (ì„¸ì…˜ ìƒíƒœë¥¼ ì‚¬ìš©í•˜ê¸° ì „ì—!)
# ---------------------------------------
if "vectorstore" not in st.session_state:
    st.session_state["vectorstore"] = None
if "chat_history" not in st.session_state:
    st.session_state["chat_history"] = []

# ---------------------------------------
# ì‚¬ì´ë“œë°”: ëª¨ë¸/ì˜µì…˜
# ---------------------------------------
with st.sidebar:
    st.markdown("### âš™ï¸ ì˜µì…˜")
    model_name = "gpt-5"
    st.markdown("âš™ï¸ LLM ëª¨ë¸: gpt-5")
    k_ctx = st.slider("ê²€ìƒ‰ ë¬¸ì„œ ìˆ˜(k)", 2, 8, 4, 1)
    chunk_size = st.slider("ì²­í¬ í¬ê¸°", 500, 2000, 1000, 100)
    chunk_overlap = st.slider("ì˜¤ë²„ëž©", 50, 400, 150, 25)
    st.markdown("---")
    st.markdown("**íŒŒì¼ ì—…ë¡œë“œ í›„, [ì¸ë±ìŠ¤ ìƒì„±]ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.**")


# ---------------------------------------
# ê³µìš©: ì—…ë¡œë“œ íŒŒì¼ì„ ìž„ì‹œê²½ë¡œë¡œ ì €ìž¥
# ---------------------------------------
def _save_uploaded_to_temp(uploaded_file, suffix):
    """Streamlit UploadedFile -> temp file path"""
    tmp = tempfile.NamedTemporaryFile(delete=False, suffix=suffix)
    try:
        shutil.copyfileobj(uploaded_file, tmp)  # .read()/getvalue() ëŒ€ì‹  ìŠ¤íŠ¸ë¦¼ ë³µì‚¬
        tmp.flush()
        return tmp.name
    finally:
        tmp.close()


# ---------------------------------------
# í•¨ìˆ˜: ë¬¸ì„œ ë¡œë”© (PDF/Text ëª¨ë‘ ìž„ì‹œê²½ë¡œ ê²½ìœ )
# ---------------------------------------
def load_docs(uploaded_files):
    docs = []
    for f in uploaded_files:
        suffix = os.path.splitext(f.name)[1].lower()

        if suffix == ".pdf":
            tmp_path = _save_uploaded_to_temp(f, ".pdf")
            try:
                loader = PyPDFLoader(tmp_path)
                loaded = loader.load()
                for d in loaded:
                    d.metadata["display_name"] = f.name
                docs.extend(loaded)
            finally:
                os.unlink(tmp_path)

        elif suffix in [".txt", ".md"]:
            tmp_path = _save_uploaded_to_temp(f, suffix)
            try:
                loader = TextLoader(tmp_path, encoding="utf-8")
                loaded = loader.load()
                for d in loaded:
                    d.metadata["display_name"] = f.name
                docs.extend(loaded)
            finally:
                os.unlink(tmp_path)

        else:
            st.warning(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” í˜•ì‹: {f.name}")

    return docs


# ---------------------------------------
# í•¨ìˆ˜: ì²­í¬ ë¶„í• 
# ---------------------------------------
def split_docs(docs, chunk_size=1000, chunk_overlap=150):
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        separators=["\n\n", "\n", " ", ""],
    )
    return splitter.split_documents(docs)


# ---------------------------------------
# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ìš•ì‹¤ ê³µì‚¬ ì‹œë°©ì„œ ì „ìš©)
# ---------------------------------------
SYSTEM_INSTRUCTIONS = """\
ë„ˆëŠ” ìš•ì‹¤(UBR) ê³µì‚¬ ì‹œë°©ì„œ ì „ìš© ì „ë¬¸ê°€ ì–´ì‹œìŠ¤í„´íŠ¸ë‹¤.
- ë°˜ë“œì‹œ ì—…ë¡œë“œëœ ì‹œë°©ì„œ/ë„ë©´(ì»¨í…ìŠ¤íŠ¸)ì— ê·¼ê±°í•´ ëŒ€ë‹µí•˜ë¼.
- ê·¼ê±°ê°€ ë¶ˆì¶©ë¶„í•˜ë©´ 'í•´ë‹¹ì‚¬í•­ ì—†ìŒ' ë˜ëŠ” 'ì‹œë°©ì„œì— ëª…ì‹œ ì—†ìŒ'ì´ë¼ê³  ë‹µí•˜ê³  ì¶”ì¸¡í•˜ì§€ ë§ˆë¼.
- ì§ˆë¬¸ì´ ì‹œë°©ì„œ ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ë©´ 'ë³¸ ì±—ë´‡ì€ ì‹œë°©ì„œ ê¸°ë°˜ ì§ˆì˜ë§Œ ë‹µë³€í•©ë‹ˆë‹¤'ë¼ê³  ì•ˆë‚´í•˜ë¼.
- ìˆ˜ëŸ‰ì´ë‚˜ ì¹˜ìˆ˜ ê³„ì‚°ì´ í•„ìš”í•œ ê²½ìš°, ë¬¸ì„œ ê·¼ê±°(íŽ˜ì´ì§€/ë¬¸êµ¬)ë¥¼ ìš”ì•½í•´ì„œ í•¨ê»˜ ì œì‹œí•˜ë¼.
- ë‹µë³€ì€ í•œêµ­ì–´ë¡œ, í•­ëª©í˜•/í‘œí˜• ì •ë¦¬ ì„ í˜¸.
"""

USER_PROMPT = ChatPromptTemplate.from_messages(
    [
        ("system", SYSTEM_INSTRUCTIONS),
        (
            "human",
            """\
ë‹¤ìŒì€ ê²€ìƒ‰ëœ ì‹œë°©ì„œ ì»¨í…ìŠ¤íŠ¸ìž…ë‹ˆë‹¤. ì´ë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µí•˜ë¼.

[ì»¨í…ìŠ¤íŠ¸]
{context}

[ëŒ€í™” ížˆìŠ¤í† ë¦¬ ìš”ì•½]
{chat_history}

[ì§ˆë¬¸]
{question}

ìš”êµ¬ì‚¬í•­:
- ë¬¸ì„œ ê·¼ê±°ì˜ í•µì‹¬ ë¬¸êµ¬ë¥¼ ì¸ìš©(ìš”ì•½)í•˜ê³ , ê°€ëŠ¥í•œ ê²½ìš° íŽ˜ì´ì§€/ì„¹ì…˜ì„ í•¨ê»˜ ì œì‹œ.
- ëª¨í˜¸í•˜ë©´ ëª…ì‹œì ìœ¼ë¡œ 'í•´ë‹¹ì‚¬í•­ ì—†ìŒ' ê¸°ìž¬.
- ìµœì¢…ì— 'ìš”ì•½' ì„¹ì…˜ìœ¼ë¡œ 3ì¤„ ì´ë‚´ í•µì‹¬ë§Œ ìž¬ì •ë¦¬.
""",
        ),
    ]
)

# ---------------------------------------
# ì—…ë¡œë”/ì¸ë±ì„œ
# ---------------------------------------
st.subheader("1) ì‹œë°©ì„œ ì—…ë¡œë“œ")
uploaded = st.file_uploader(
    "PDF(.pdf) ë˜ëŠ” í…ìŠ¤íŠ¸(.txt/.md) ì‹œë°©ì„œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš” (ë³µìˆ˜ ê°€ëŠ¥)",
    type=["pdf", "txt", "md"],
    accept_multiple_files=True,
)

col_a, col_b = st.columns(2)
with col_a:
    if st.button("ðŸ“š ì¸ë±ìŠ¤ ìƒì„±", use_container_width=True, type="primary"):
        if not uploaded:
            st.warning("ë¨¼ì € íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.")
        else:
            with st.spinner("ë¬¸ì„œ ë¡œë”©/ì²­í¬ ë¶„í• /ìž„ë² ë”© ì¤‘..."):
                raw_docs = load_docs(uploaded)
                chunks = split_docs(
                    raw_docs, chunk_size=chunk_size, chunk_overlap=chunk_overlap
                )
                embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
                vs = FAISS.from_documents(chunks, embeddings)
                st.session_state["vectorstore"] = vs
            st.success(f"ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ! (ì²­í¬ ìˆ˜: {len(chunks)})")

with col_b:
    if st.button("ðŸ—‘ ì¸ë±ìŠ¤ ì´ˆê¸°í™”", use_container_width=True):
        st.session_state["vectorstore"] = None
        st.session_state["chat_history"] = []
        st.success("ì´ˆê¸°í™” ì™„ë£Œ.")


# ---------------------------------------
# RAG ì²´ì¸ êµ¬ì„±
# ---------------------------------------
def make_rag_chain(vectorstore):
    retriever = vectorstore.as_retriever(
        search_type="mmr", search_kwargs={"k": k_ctx, "fetch_k": max(10, k_ctx * 4)}
    )
    llm = ChatOpenAI(model=model_name)

    def format_docs(docs):
        formatted = []
        for d in docs:
            src_path = d.metadata.get("source", "")
            page = d.metadata.get("page", None)
            disp = d.metadata.get(
                "display_name", os.path.basename(src_path) or "document"
            )
            head = f"[source: {disp}"
            if page is not None:
                head += f", page: {page+1}"
            head += "]"
            formatted.append(f"{head}\n{d.page_content}")
        return "\n\n---\n\n".join(formatted)

    rag = (
        {
            # âœ… retrieverì—ëŠ” 'ì§ˆë¬¸' ë¬¸ìžì—´ë§Œ í˜ë ¤ë³´ë‚´ê¸°
            "context": (lambda x: x["question"]) | retriever | format_docs,
            "question": lambda x: x["question"],
            "chat_history": lambda x: x["chat_history"],
        }
        | USER_PROMPT
        | llm
    )
    return rag, retriever


# ---------------------------------------
# ì§ˆì˜ ì˜ì—­
# ---------------------------------------
st.subheader("2) ì§ˆë¬¸í•˜ê¸°")
q = st.text_input(
    "ì‹œë°©ì„œ ê´€ë ¨ ì§ˆë¬¸ì„ ìž…ë ¥í•˜ì„¸ìš” (ì˜ˆ: 'UBR ê³µì‚¬ì—ì„œ ë²½ì²´ íƒ€ì¼ ê·œê²©ì€?')"
)

if st.session_state["vectorstore"] is None:
    st.info("ë¨¼ì € ì‹œë°©ì„œë¥¼ ì—…ë¡œë“œí•˜ê³  ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•˜ì„¸ìš”.")
else:
    rag_chain, retriever = make_rag_chain(st.session_state["vectorstore"])

    if st.button("ðŸ”Ž ì§ˆì˜ ì‹¤í–‰", type="primary") and q.strip():
        with st.spinner("ê²€ìƒ‰ ë° ë‹µë³€ ìƒì„± ì¤‘..."):
            # Deprecated API êµì²´: get_relevant_documents -> invoke
            docs = retriever.invoke(q)

            # Runnable ë°”ê¹¥ì—ì„œ ì•ˆì „í•˜ê²Œ chat_history ë¬¸ìžì—´ ìƒì„±
            chat_history_str = (
                "\n".join(
                    [
                        f"Q: {qq}\nA: {aa}"
                        for qq, aa in st.session_state["chat_history"]
                    ][-6:]
                )
                if st.session_state["chat_history"]
                else "ì—†ìŒ"
            )

            # ì²´ì¸ì— ëª…ì‹œì ìœ¼ë¡œ ìž…ë ¥ ì „ë‹¬
            answer_msg = rag_chain.invoke(
                {"question": q, "chat_history": chat_history_str}
            )

        # ížˆìŠ¤í† ë¦¬ ì €ìž¥
        st.session_state["chat_history"].append((q, answer_msg.content))

        # ì¶œë ¥
        st.markdown("### ðŸ§  ë‹µë³€")
        st.markdown(answer_msg.content)

        with st.expander("ðŸ”Ž ì‚¬ìš©í•œ ê·¼ê±°(ìƒìœ„ ê²€ìƒ‰ ê²°ê³¼) ë³´ê¸°"):
            for i, d in enumerate(docs, 1):
                src_path = d.metadata.get("source", "")
                page = d.metadata.get("page", None)
                disp = d.metadata.get(
                    "display_name", os.path.basename(src_path) or "document"
                )
                st.markdown(
                    f"**[{i}] {disp}**  (page: {page+1 if page is not None else 'N/A'})"
                )
                st.write(
                    d.page_content[:1200]
                    + ("..." if len(d.page_content) > 1200 else "")
                )

# ---------------------------------------
# ížˆìŠ¤í† ë¦¬ í‘œì‹œ
# ---------------------------------------
if st.session_state["chat_history"]:
    st.markdown("---")
    st.markdown("### ðŸ’¬ ëŒ€í™” ížˆìŠ¤í† ë¦¬")
    for i, (qq, aa) in enumerate(reversed(st.session_state["chat_history"][-8:]), 1):
        st.markdown(f"**Q{i}.** {qq}")
        st.markdown(f"**A{i}.** {aa}")
